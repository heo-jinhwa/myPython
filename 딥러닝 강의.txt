필수 1. openpyxl (Excel 자동화) 
필수 2. pyautogui (Desktop 자동화)
필수 3. pytesseract, easyocr (OCR 작업, 두개 Library중에서 성능 비교 분석해서 OCR 적용)
선택 4. pandas (엑셀작업, 데이터 정제)
선택 5. python opencv (이미지 처리, OCR작업 시 분석용으로 사용)
선택 6. python machinelearning (머신러닝, 데이터 분석하여 결과 주는 안건에 적용)

■ 딥러닝 프레임워크 : TensorFlow, Keras Pytorch 등
■ TensorFlow 
 - 추상화를 통해 알고리즘 구현
 - 다차원 데이터 배열(Tensor) + 데이터 흐름(Data Flow)
 - Keras 지원 
  -> 기존 (1.x 버전) : TensorFlow 구조와 문법으로 하나하나 작업이 필요
  -> 변경 (2.x 버전) : 쉬운 인터페이스 제공하여 Keras 프레임워크 포함

■ 기계학습
 - 지식기반 방식 한계 => 기계학습 등장 : 조건을 컴퓨터 학습모델의 가중치로 바꿔 학습

■ 지도학습
 ▶ K-NN 알고리즘 : 새로 들어온 데이터 기준으로 가장 가까운 K개 데이터 판단하여 더 많은 데이터와 같다고 판단 
                          ex) K=3인 경우 3개의 데이터를 판단 (빨강2, 파랑1)할 때, 새로운 데이터를 빨강으로 분류 
   [장점]
   높은 정확도 -> 기존 데이터 모두를 매번 새롭게 검사하기 때문
   오류 데이터 혹인 이상 데이터가 결과값에 영향 없음 -> 가장 가까운 K개의 데이터를 활용하기 때문
   [단점]
   K의 개수에 따라 결과가 달라질 수 있음
   항상 모든 데이터를 비교해야하므로 느림 (처리시간 증가)
   [실습예제]
   분류를 위한 속성값 - 크기(귤:1~10, 오렌지:7~20) / 무게 (귤:50~100, 오렌지 : 80~130)

■ 비지도학습
 - 입력 데이터만 주고 컴퓨터에게 스스로 답을 찾을 수 있는 프로그램을 작성하도록 학습시키는 방법
 - Target Value(정답)이 주어지지 않음
 - 데이터의 패턴, 구성, 특징 등을 스스로 파악
 - 클러스터링

 ▶ K-Means알고리즘 (K-평균 알고리즘) : 입력된 데이터를 K개의 클러스터(Cluster)로 묶는 방법 
  - K개의 중심점을 잡고 입력데이터를 분류 / 무게중심을 기준으로 중심점 이동으로 군집화 (중심점 이동이 완료될 때까지)

  [장점]
  간단한 알고리즘 구현
  입력데이터에 Target value 필요 없음

  [단점]
  K를 직접 지정, K의 개수에 따라 결과가 달라짐
  오류데이터, 이상 데이터에 민감 -> 데이터의 평균을 구해야함

■ 인공신경망
 ▶ 퍼셉트론 : 딥러닝의 기원, 여러 개의 입력을 받고 하나의 출력을 내보낼 수 있음 -> 다음 뉴런으로 정보를 전달할지 여부 결정
y = 0 (w1x1+w2x2 <= @, bias) 흐르지않는다 -> 입력*가중치+편향(b)<=0
y = 1 (w1x1+w2x2 > @) 흐른다 -> 입력*가중치+편향(b)>0
가중치가 클수록 입력 결과에 많은 영향 / 입력이 전체 인공신경망에서 중요한 역할

■ 데이터셋
 데이터 수집, 분석, 가공 필요
 ▶ 데이터셋의 종류
  - 학습을 위한 훈련 데이터셋
  - 검증을 위한 검증 데이터셋
  - 테스트를 위한 시험 데이터셋

■ 딥러닝 학습 과정 : 데이터 준비 ▶ 모델준비 ▶ 학습과정준비 ▶ 학습 및 평가 ▶ 분석 및 활용
■ 딥러닝 모델 준비
 - MLP(Multi Layer Perceptron, 다층 퍼셉트론) 활용 -> 순차적으로 층을 쌓아나가기 때문에 Sequential 모델 사용
 - 하나의 층을 쌓아 연결을 해주는 Dense : Dense(출력 뉴런 수, input_dim=입력 뉴런 수, activation=활성화 함수)
 - XOR 예시 
 0층 : 입력이 2개
 1층 : 0층 출력 2개가 다시 입력
 2층 : 출력 1개
model = Sequential()
model.add(Dense(2, input_dim=2, activation='sigmoid'))
model.add(Dense(1, activation='sigmoid')) - 순차적으로 하나씩 층이 추가되기 때문에 자동으로 인식해서 적용

■ 딥러닝 학습 과정 준비
 - 완성한 모델을 조립하여 학습하는 방법을 지정 -> 학습 할 모델의 손실 함수 및 옵티마이저, 평가지표 등을 설정
model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])

■ 준비된 모델 학습
hist = model.fit(X_train, Y_train, epochs=200, batch_size=10, validation_data=(X_val, Y_val))
※ 배치사이즈 : 한 번에 몇 문제씩 풀지 지정 : 문제를 다 풀고 틀린 문제를 맞춰보며 가중치 갱신으로 모델을 향상시킴
예시)
문제 100개 / 정답 100개
batch_size 10이면 10문제 풀고 가중치 갱신 10회 반복
batch_size 100이면 100문제 풀고 가중치 갱신 1회 반복

[batch_size 클때 특징]
 - 학습 속도가 빠르다 : 가중치 갱신을 자주 하지 않기 때문
 - 많은 메모리, 컴퓨팅 성능 필요 : 한번에 많은 문제를 풀고 문제에 대한 정답들을 기억/학습 해야함

[batch_size 작을때 특징]
 - 학습 속도가 느리다 : 가중치 갱신을 자주 하기 때문
 - 많은 메모리와 컴퓨팅 성능 불필요 

■ 성능평가
Loss(손실) : 낮을 수록 모델의 성능이 좋음
Accuracy(정확도) : 높을 수록 모델의 성능이 좋음
loss_and_metrics = model.evaluate(X_text, Y_text, batch_size=1)
loss_and_metrics = [loss, accuracy]로 구성

■ 딥러닝 평가
 분석 및 활용 방법의 대표적 방법은 그래프로 시각화 matplotlib.pyplot as plt 

■ 딥러닝 학습모델 원리
 정답이 2라면, 2인 값이 가장 큰 값을 가져야함 (One-Hot-Encoding)
 입력값이 2일 때, y1~y10중 y3값은 1 나머지 y는 모두 0으로 예측
 One-Hot-Encoding이 적용된 정답과 모델이 예측한 값의 차이를 줄여나가는 과정

 학습 : 모델이 예측한 값과 실제 정답을 비교하며 손실을 줄이는 것

■ 오차 역전파 
 모델의 예측 값과 실제 정답과 손실을 줄이기 위해 틀린 것을 모델에게 다시 알려주고 수정하는 과정
  -> 모델을 구성하고 있는 인공신경망과 가중치와 바이어스 값을 맞출 수 있도록 계산해서 수정하는 과정
 여러 층의 인공신경망을 거치면서 각각의 가중치가 곱해지며 최종 출력을 만들어감
  -> 가중치가 높다는 것은 출력 값에 많은 영향을 미친 것이랏 오차를 알려줄 때도 가중치를 반영하여 알려줘야함

* 모델이 예측한 결과의 손실 값(오차)가 0.6일 때, 거꾸로 신경망을 돌아가며(역으로) 각각의 가중치에 맞게 손실값 전달

■ 경사하강법
 효율적으로 오차를 줄여나가기 위한 방법
 함수의 기울기를 구하여 기울기가 낮은 곳으로 계속 이동시키면서 극값에 이르게 하는 최적화된 알고리즘
 학습을 위해 손실 값을 최소화하기 위한 방법으로 사용 -> 오차가 가장 작은 방향으로 이동시키는 방법 (기울기가 0인 지점을 찾아가는 것)

예시) 선형회귀
아파트 크기에 따른 매매 가격 분석은 하나의 선으로 모델 분석 가능
1) 임의의 선을 긋고 크기별 가격 데이터와의 오차를 계산하여 오차를 줄여 최적의 선을 찾음
2) 컴퓨터는 y=ax+b에서 오차를 최소화하기 위한 a와 b를 찾아야함

-> 경사하강은 최적의 값을 찾아가는 과정에서 경사 기울기가 점점 하강하기 때문

★ 컴파일 함수에서 옵티마이저는 경사하강법을 의미함 (오차 계산 방식, 최적화 방식 등)
 loss - 손실함수의 오차를 줄여 나가기 위함
 binary - 둘 중 하나인 바이너리 값을 정답으로 출력값을 찾아낼 때 Loss를 줄이기 위함
 category - 여러 개중 정답을 찾는것
★ 옵티마이저 - 오차를 계산해 최적 값을 찾아가는 방법 (경사하강법)

경사하강법 / 관성법칙 등 -> 더 빨리 오차계산을 하기 위한 방식을 다양하게 적용

■ 텐서플로우 모듈 - 답러닝 학습을 위해 다양한 기능을 제공
1. 콜백함수 : 조기종료에 사용
   - 딥러닝 모델 학습 -> 반복 -> 모델 과적합 발생
   - 과적합 방지를 위한 조기종료 함수 제공
   - EarlyStopping 모델이 더 이상 개선의 여지가 없다고 판단할 때 학습을 종료하는 콜백 함수
   - 학습이 잘 되고 있다고 판단하는 기준 : loss(손실)값이 반복마다 점점 떨어지는 것 
   - 과적합이 되었다고 판단하는 기준 : loss(손실)값이 다음 반복에 오히려 올라감
   -> 특정 함수를 수행할 때 해당 함수에서 내가 지정한 함수를 호출
   - 어렵고 복잡한 문제의 경우 loss 값이 불안정하게 감소할 수 있음 (특히 학습 초반) -> 학습이 덜 됐지만 우연히 정답을 맞춘 경우 학습이 종료되는 현상
   -> patience 옵션 사용 : 몇 번 연속 올라가지 않으면 일시적인 경우로 보고 다시 학습 진행
2. 멀티레이어 퍼셉트론

인공신경망 -> 다층 퍼셉트론(MNIST모델)
텐서플로우 모델의 종류
- 인공신경망의 기본 모델인 다층 퍼셉트론
- CNN모델 : 이미지 처리에 적합
- RNN모델 : 자연어 처리에 적합

■ CIFAR-10 데이터셋 : 인공지능 모델 학습용으로 제공되는 데이터 셋 / MINST데이터셋보다 난이도가 높음 / 10종류의 컬러 이미지 제공(CIFAR-10) 100종류인 CIFAR-100도 있음


========================================================================================================================================================================

[자연어 처리 강의]

자연어 처리 학습 절차
1. 텍스트 전처리 -> 2. 네트워크 모델 설계 -> 3. 모델 훈련 -> 4. 모델 예측 및 평가

■ 텍스트 전처리 : 정제, 불용어 제거, 어간 추출, 토큰화 및 문서 표현
코퍼스 (Corpus) : 여러 문장들로 구성된 문서의 집합

■ torchtext : 텍스트 전처리를 위한 파이썬 라이브러리 (Pytorch)

정제 (Cleaning) : 특수문자 등과 같은 불필요한 노이즈 텍스트 제거 및 대소문자 통일 
불용어제거 (Stop word Elimination) : 전치사, 관사 등 문장이나 문서의 특징을 표현하는데 불필요한 단어 제거
어간 추출 (Stemming) : 단어의 기본 형태를 추출하는 단계
토큰화 (Tokenization) : 코퍼스에서 분리자(Separator)를 포함하지 않는 연속적인 문자열 단위로 분리 
문서 표현 (Representation) : 주어진 문서나 문장을 하나의 벡터로 표현하는 단계 -> 단어들을 모두 인덱싱하고 주어진 문서에 존재하는 단어의 빈도수를 사용하여 문서 표현
-> 문서표현방법 : Ont-hot-encoding / Word2Vec 등

네트워크 모델 설계
Functional API 구조 : 다중 입출력 및 분기 흐름의 모델 구성에 사용되는 구조

■ 데이터스케일링 : 특성별로 데이터의 스케일이 다르다면 머신러닝 동작에 제한 -> 모든 특성의 범위를 같게 만들어야함
-> Standardization (표준화) : 특성들의 평균을 0, 분산을 1로 스케일링 -> 특성들을 정규분포화 
-> Normalization (정규화) : 특성들을 특정 범위 (주로[0,1])로 스케일링 하는 것 

